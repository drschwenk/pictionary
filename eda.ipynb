{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Table of Contents\n",
    "* [1. load](#1.-load)\n",
    "* [2. simple words](#2.-simple-words)\n",
    "* [3. filtering sample](#3.-filtering-sample)\n",
    "* [4. tag sentences](#4.-tag-sentences)\n",
    "* [5. parse npc extraction](#5.-parse-npc-extraction)\n",
    "* [6. clean and spellcheck](#6.-clean-and-spellcheck)\n",
    "\t* [6.1 grammer experiment](#6.1-grammer-experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import itertools\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from IPython.display import Image\n",
    "import PIL.Image as pil\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"Agg\")\n",
    "import matplotlib.pylab as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext base16_mplrc\n",
    "%base16_mplrc light solarized\n",
    "#%base16_mplrc dark solarized\n",
    "plt.rcParams['grid.linewidth'] = 0\n",
    "plt.rcParams['figure.figsize'] = (16.0, 10.0)\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from nltk.draw.tree import TreeView\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tree import ParentedTree\n",
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from word_mod import zipf_frequency\n",
    "import language_check\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from anigen_tools.parsing import parse_description\n",
    "from anigen_tools.parsing import sanitize_text\n",
    "from anigen_tools.parsing import sentence_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "punct_set = set(string.punctuation)\n",
    "punct_set.remove('.')\n",
    "cached_sw = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "core_nlp_base = '/Users/schwenk/wrk/animation_gan/phrase_cues/deps/stanford_core_nlp/stanford-corenlp-full-2017-06-09/'\n",
    "core_parser = CoreNLPParser(url='http://localhost:9000')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "out_data_dir = '/Users/schwenk/wrk/pictionary/phrase_gen/sandbox-anikem/out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verb_df = pd.read_csv(out_data_dir + 'verbs.txt', header=None, names=['verb'])\n",
    "situations_df = pd.read_csv(out_data_dir + 'full_situations.txt', header=None, names=['situation'])\n",
    "\n",
    "situations_df['len'] = situations_df['situation'].apply(lambda x: len(x.split()))\n",
    "verb_df['len'] = verb_df['verb'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>situation</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the person loads a cartridge holder</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             situation  len\n",
       "0  the person loads a cartridge holder    6"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "situations_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# _ = situations_df['len'].hist(bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# verb_df.sort_values('len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# ing_verbs = verb_df[verb_df['verb'].str.contains('ing')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "not abstract, easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. simple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "word_data_dir = '/Users/schwenk/wrk/pictionary/BasicEnglishTranslator/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/basic_words.csv') as f:\n",
    "    read_article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open(word_data_dir + 'basic_english.pickle', 'rb') as f:\n",
    "    all_words = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "simple_words = pd.read_csv(word_data_dir + 'basic_english_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WORD\n",
       "0    a"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_words.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. filtering sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "gram_checker = language_check.LanguageTool('en-US')\n",
    "gram_checker.disabled = set(['UPPERCASE_SENTENCE_START'])\n",
    "# gram_checker.disable_spellchecking()\n",
    "\n",
    "def grammar_check_phrases(dataset):\n",
    "    corrected = {}\n",
    "    passed = []\n",
    "    for sentence in dataset:\n",
    "        corrections = gram_checker.check(sentence)\n",
    "        if corrections:\n",
    "            corrected[sentence] = corrections\n",
    "        else:\n",
    "            passed.append(sentence)\n",
    "    return corrected, passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "description_corpus = ' '.join(situations_df['situation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = word_tokenize(description_corpus)\n",
    "\n",
    "normalized_tokens = []\n",
    "for toke in tokenized_corpus:\n",
    "    clean_toke = toke.strip().lower()\n",
    "    if clean_toke not in cached_sw and clean_toke:\n",
    "        normalized_tokens.append(clean_toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(normalized_tokens)\n",
    "most_common = word_counts.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "filtered_words = [w for w, count in word_counts.items() if count == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = list(set(situations_df['situation'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "test_sent = sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# sent_gram_errors, passed_sents = grammar_check_phrases(tqdm(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9645086556406461"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(passed_sents) / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59950"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(passed_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. tag sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_pos_tags(sentence):\n",
    "    constituent_parse  = [list(i)[0] for i in core_parser.raw_parse_sents([sentence])][0]\n",
    "    pos_tags = constituent_parse.pos()\n",
    "    return pos_tags\n",
    "\n",
    "def filter_pos_tags(tagged_sent):\n",
    "    include_as_article = []\n",
    "    for word, tag in tagged_sent:\n",
    "        if tag == 'DT':\n",
    "            include_as_article.append('True')\n",
    "        else:\n",
    "            include_as_article.append('False')\n",
    "    return include_as_article\n",
    "\n",
    "def least_common_word(sent):\n",
    "    wlfs = [zipf_frequency(w, 'en') for w in sent.split()]\n",
    "    lowest_freq = min(wlfs)\n",
    "    return lowest_freq > 4.7\n",
    "\n",
    "def form_strings(tagged_sentence):\n",
    "    words = [w[0] for w in tagged_sentence]\n",
    "    include_word = filter_pos_tags(tagged_sentence)\n",
    "    return f\"{' '.join(words)} # {' '.join(include_word)}\"\n",
    "\n",
    "def form_strings_w_pos(tagged_sentence):\n",
    "    words = [w[0] for w in tagged_sentence]\n",
    "    include_word = [w[1] for w in tagged_sentence]\n",
    "    return f\"{' '.join(words)} # {' '.join(include_word)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('unique_parsed_sents.pkl', 'rb') as f:\n",
    "    unique_parsed_sents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "simple_sentences = [sent for sent in unique_parsed_sents if least_common_word(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# with open('unique_parsed_sents.pkl', 'wb') as f:\n",
    "#     pickle.dump(unique_parsed_sents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "some_sents = unique_parsed_sents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tagged_sentences = [get_pos_tags(s) for s in simple_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_complete = list(set(normalized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "vocab = list(set([w[0].lower() for s in tagged_sentences for w in s if w[1] in ['NN', 'NNS', 'NNP', 'NNPS']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "voc_ser = pd.Series(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "voc_ser.to_csv('cleaned_sentences_nouns.txt', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_ser.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.22"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw = vocab_complete[0]\n",
    "zipf_frequency(vw, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bald eagle is swooping in blue sky'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_parsed_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1405"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "build_output = pd.Series([form_strings_w_pos(s) for s in tagged_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "build_output.to_csv('cleaned_sentences_simple_pos_tags.txt', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# def custom_filter_sentences(sentences):\n",
    "#     required = ['a ', 'the ']\n",
    "#     filtered = []\n",
    "#     for sent in sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5. parse npc extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "s1 = constituent_parse[1]\n",
    "TreeView(s1)._cframe.print_to_file('s2.ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2231,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The red-haired businessman\n",
      "accountant\n",
      "the phone\n",
      "his office\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "  {<NNP>+}                # chunk sequences of proper nouns\n",
    "  {<NN>+}                 # chunk consecutive nouns\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "parsed_sent = cp.parse(sent_parses[0])\n",
    "for npstr in extract_np(parsed_sent):\n",
    "    print (npstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_np(psent):\n",
    "    for subtree in psent.subtrees():\n",
    "        if subtree.label() == 'NP':\n",
    "            subprod = subtree.productions()[0].unicode_repr()\n",
    "            if 'NN' in subprod or 'NNP' in subprod:\n",
    "                yield ' '.join(word for word in subtree.leaves())\n",
    "\n",
    "\n",
    "def compute_token_spans(const_parse_sent, txt):\n",
    "    tokens = const_parse_sent.leaves()\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "\n",
    "        \n",
    "def assign_word_spans(noun_phrases_w_spans, doc, token_spans):\n",
    "    chunk_spans = []\n",
    "    for np in noun_phrases_w_spans:\n",
    "        char_span = [(m.start(), m.end()) for m in re.finditer(np, doc)][0]\n",
    "        start, end = char_span\n",
    "        start_w, end_w = None, None\n",
    "        for w_idx, token_span in enumerate(token_spans):\n",
    "            token, ts, te = token_span\n",
    "            if ts == start:\n",
    "                start_w = w_idx\n",
    "            if te == end:\n",
    "                end_w = w_idx + 1\n",
    "        if type(start_w) == int and type(end_w) == int:\n",
    "            chunk_spans.append([start_w, end_w])\n",
    "        else:\n",
    "            print('failed')\n",
    "    return chunk_spans\n",
    "\n",
    "\n",
    "def np_chunker(doc, parsed_sents):\n",
    "    noun_phrases = [list(extract_np(sent)) for sent in parsed_sents]\n",
    "#     noun_phrase_spans = [list(extract_np_spans(doc, sent)) for sent in noun_phrases]\n",
    "    token_spans = [list(compute_token_spans(sent, doc)) for sent in parsed_sents]\n",
    "    noun_phrase_spans = [assign_word_spans(noun_phrases[n], doc, token_spans[n]) for n in range(len(parsed_sents))]\n",
    "    return {'chunks': noun_phrase_spans, 'named_chunks': noun_phrases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 6. clean and spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "# import difflib\n",
    "# import diff_match_patch\n",
    "\n",
    "# dmp = diff_match_patch.diff_match_patch()\n",
    "\n",
    "edict = enchant.Dict(\"en_US\")\n",
    "anglo_edict = enchant.Dict(\"en_UK\")\n",
    "cached_sw = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "main_characters = {\n",
    "}\n",
    "\n",
    "other_chars_names = []\n",
    "\n",
    "other_words = []\n",
    "\n",
    "words_to_remove = []\n",
    "\n",
    "_ = [edict.remove_from_session(word) for word in words_to_remove]\n",
    "_ = [[edict.add(char_word.lower()) for char_word in char.split()] for char in list(main_characters) + other_chars_names + other_words]\n",
    "\n",
    "_ = [anglo_edict.remove_from_session(word) for word in words_to_remove]\n",
    "\n",
    "manual_corrections = {\n",
    "                     }\n",
    "\n",
    "def check_mispelled(word):\n",
    "    return word and word.isalpha() and not (edict.check(word))\n",
    "\n",
    "def check_word_rules(word):\n",
    "    split_len = 2 < min([len(w) for w in word.split()])\n",
    "    not_proper = word[0].islower()\n",
    "    return not_proper and split_len\n",
    "\n",
    "def correct_spelling_error(misspelled_word):\n",
    "    if misspelled_word in manual_corrections:\n",
    "        return manual_corrections[misspelled_word]\n",
    "    suggested_spellings = edict.suggest(misspelled_word)\n",
    "    match_ratios = [fuzz.token_sort_ratio(misspelled_word, word) for word in suggested_spellings]\n",
    "    words_sorted_by_ratio = sorted(zip(suggested_spellings, match_ratios), key=lambda x: x[1], reverse=True)\n",
    "    words_sorted_by_ratio = [wordscore for wordscore in words_sorted_by_ratio if check_word_rules(wordscore[0])]\n",
    "    check_compounds = [word[0] for word in words_sorted_by_ratio if word[0].replace(' ', '') == misspelled_word]\n",
    "    if check_compounds:\n",
    "        return check_compounds[0]\n",
    "    if words_sorted_by_ratio[0][1] > 80:\n",
    "        return words_sorted_by_ratio[0][0]\n",
    "\n",
    "    for word, score in words_sorted_by_ratio:\n",
    "        if score >= 75 and word[0] == misspelled_word[0]:\n",
    "            return word\n",
    "        elif score >= 75:                    \n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def apply_spelling_fix(orig_text):\n",
    "    orig_text_tokens = wordpunct_tokenize(orig_text)\n",
    "    processed_tokens = []\n",
    "    for token in orig_text_tokens:\n",
    "        norm_token = token.lower()\n",
    "        if len(norm_token) < 4:\n",
    "            processed_tokens.append(token)\n",
    "            continue\n",
    "        if check_mispelled(norm_token):\n",
    "            suggested_replacements = edict.suggest(token)\n",
    "            replacement_text = correct_spelling_error(norm_token, suggested_replacements)\n",
    "            if replacement_text:\n",
    "                if norm_token[0].isupper():\n",
    "                    replacement_text = upper(replacement_text[0]) + replaced_text[1:]\n",
    "                processed_tokens.append(replacement_text)\n",
    "            else:\n",
    "                processed_tokens.append(token)\n",
    "        else:\n",
    "            processed_tokens.append(token)\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def diff_corrected_text(orig_text, corrected_text):\n",
    "    diff = dmp.diff_main(orig_text, corrected_text)\n",
    "    return HTML(dmp.diff_prettyHtml(diff))\n",
    "\n",
    "def specify_lesson_q_path(lesson):\n",
    "    pass\n",
    "\n",
    "def apply_spelling_and_grammar_to_ds(ck12_ds):\n",
    "    return\n",
    "\n",
    "def spellcheck_entry(entry_words):\n",
    "    misspellings = [check_mispelled(word) for word in entry_words]\n",
    "    if sum(misspellings):\n",
    "        for idx, is_mispelled in enumerate(misspellings):\n",
    "            if is_mispelled:\n",
    "                suggested_replacement = correct_spelling_error(entry_words[idx])\n",
    "                if suggested_replacement:\n",
    "#                     words_changed.append([char_name_words[idx], suggested_replacement])\n",
    "                    entry_words[idx] = suggested_replacement\n",
    "        return entry_words\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def spellcheck_video(video):\n",
    "    corrected_words = []\n",
    "    for entity in video._data['characters'] + video._data['objects']:\n",
    "        if 'entityLabel' not in entity.data().keys():\n",
    "            pass\n",
    "        entry_words = wordpunct_tokenize(entity.data()['entityLabel'])\n",
    "#         print(entry_words)\n",
    "        new_corrected_words = spellcheck_entry(entry_words)\n",
    "        if new_corrected_words:\n",
    "            entity._data['originalSpelling'] = entity._data['entityLabel']\n",
    "            entity._data['entityLabel'] = ' '.join(new_corrected_words).replace(' , ', ', ').replace(' . ', '.')\n",
    "            corrected_words.append(new_corrected_words)\n",
    "    description_words = wordpunct_tokenize(video.description())\n",
    "    new_corrected_words = spellcheck_entry(description_words)\n",
    "    if new_corrected_words:\n",
    "        video._data['originalDescription'] = video._data['description']\n",
    "        video._data['description'] = ' '.join(new_corrected_words).replace(' , ', ', ').replace(' . ', '.')\n",
    "        corrected_words.append(new_corrected_words)\n",
    "    return corrected_words\n",
    "\n",
    "def spellcheck_dataset(dataset):\n",
    "    corrected = {}\n",
    "    for vid in dataset:\n",
    "        try:\n",
    "            corrections = spellcheck_video(vid)\n",
    "            if corrections:\n",
    "                corrected[vid.gid()] = corrections\n",
    "        except:\n",
    "            pass\n",
    "#             print(vid.gid())\n",
    "    return corrected\n",
    "\n",
    "def grammar_check_video(video):\n",
    "    all_errors = []\n",
    "    for entity in video._data['characters']:\n",
    "        errors = gram_checker.check(entity.data()['entityLabel'])\n",
    "        if errors:\n",
    "            all_errors.append(errors)\n",
    "    return all_errors\n",
    "\n",
    "def grammar_check_dataset(dataset):\n",
    "    corrected = {}\n",
    "    for vid in dataset:\n",
    "        corrections = grammar_check_video(vid)\n",
    "        if corrections:\n",
    "            corrected[vid.gid()] = corrections\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 6.1 grammar experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import language_check\n",
    "\n",
    "gram_checker = language_check.LanguageTool('en-US')\n",
    "gram_checker.disabled = set(['UPPERCASE_SENTENCE_START'])\n",
    "gram_checker.disable_spellchecking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "desc_with_errors = {}\n",
    "corrected_desc = {}\n",
    "for diag, desc_text in diagram_desc_dict.items():\n",
    "    errors = gram_checker.check(desc_text)\n",
    "    if errors:\n",
    "        desc_with_errors[diag] = errors\n",
    "        corrected_desc[diag] = gram_checker.correct(desc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import viterbi_tagger, perceptron_tagger, cfg\n",
    "\n",
    "# test values for developers\n",
    "test_sentence = [\"I\", \"saw\", \"the\", \"duck\"]\n",
    "cfg_test = [\"NNP\",\"VBD\",\"NNP\"]\n",
    "from nltk.corpus import conll2000, brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Build HMM\n",
    "print(\"Generating Hidden Markov Model...\")\n",
    "viterbi_tagger = viterbi_tagger.PartOfSpeechTagger()\n",
    "# Build probability distributions for each of the corpora we want to use\n",
    "print(\"Building POS tag probability distributions based on...\")\n",
    "print(\"Corpora 1: Conll2000,\")\n",
    "viterbi_tagger.buildProbDist(conll2000)\n",
    "print(\"Corpora 2: Brown.\")\n",
    "viterbi_tagger.buildProbDist(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Train the AP Tagger weights\n",
    "print(\"Prepare Averaged Perceptron tagger based on tagged corpora\")\n",
    "# taggerAP = perceptron_tagger.AP_Tagger(False)\n",
    "#\n",
    "# Build CFG rule set based on treebank\n",
    "print(\"Generating Context Free Grammar based on Treebank...\")\n",
    "cfg_checker = cfg.Grammar()\n",
    "tbank_grammar = cfg_checker.buildFromTreebank()\n",
    "#\n",
    "# Loop input to get and check sentences\n",
    "print(\"If an input word is not in the corpora, the averaged perceptron \\\n",
    "tagger will be used instead of the Viterbi tagger.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     tag_sequence = viterbi_tagger.inputToPOS()\n",
    "#     print(\"TAG SEQUENCE:\", tag_sequence)\n",
    "#     cfg_checker.verify(tbank_grammar, tag_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
